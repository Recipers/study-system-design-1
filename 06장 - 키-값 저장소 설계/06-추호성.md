# 6. 키-값 저장소 설계

키-값 저장소는 키를 고유 식별자로 갖는 비 관계형 데이터베이스다. 키는 짧을수록 좋다.

## 문제 이해 및 설계 범위 확정

읽기, 쓰기 그리고 메모리 사용량 사이에서 균형을 찾고, 데이터의 일관성과 가용성 사이에서 결정을 내려야 한다. 다음과 같은 DB를 설계해보자.

- 키-값 쌍의 크기는 10KB 이하다.
  
- 큰 데이터를 저장할 수 있어야 한다.
- 높은 가용성을 제공해야 한다. 장애가 있더라고 빨리 응답해야 한다.
- 확장성을 제공해야 한다. 트래픽 양에 따라 자동적으로 서버 증설, 삭제가 이뤄져야 한다.
- 데이터 일관성 수준은 조정이 가능해야 한다,
- 레이턴시가 짧이야 한다.

## 단일 서버 키-값 저장소

단일 서버 저장소는 빠른 속도를 보장하긴 하지만 구현의 한계점이 있다. 데이터 압축과 자주 사용하는 데이터만 메모리에 저장하는 방식으로 해결할 수 있지만, 분산 키-값 저장소가 필요한 시점이다.

## 분산 키-값 저장소

분산 해시 테이블이라고도 불린다. 분산 시스템을 설계할 때는 CAP 정리를 이해하고 있어야 한다.

### CAP 정리

CAP 정리는 일관성, 가용성, 파티션 감내라는 세 가지 요구사항을 동시에 만족하는 분산 시스템을 설계하는 것은 불가능 하다는 정리다. 보통 한 개가 포기된다.

- 데이터 일관성 : 분산 시스템에 접속하는 모든 클라이언트는 어느 곳에서든 같은 시점에 같은 데이터를 봐야 한다.
  
- 가용성 : 일부 노드에 장애가 발생하더라도 항상 응답을 받을 수 있어야 한다.
- 파티션 감내 : 파티션은 두 노드 사이에 통신 장애가 발생하였음을 의미한다. 파티션 감내는 네트워크에 파티션이 생기더라도 시스템은 계속 동작해야 한다는 것을 뜻한다.(네트워크가 분리되더라도 시스템이 동작하는 능력을 의미한다)

실세계의 분산 시스템은 파티션 문제를 피할 수 없다. 그리고 파티션에 문제가 발생하면 우리는 일관성과 가용성 사이에서 하나를 선택해야 한다.

다음 그림과 같이 n3에 장애가 발생하여 n1, n2와 통신할 수 없다고 가정해보자. 

![image](https://github.com/user-attachments/assets/bf5ec64b-ceb2-4612-acc3-34b5f62317c3)

가용성을 선택할 경우, 쓰기 및 읽기 연산이 그대로 작동한다. 단, n3 장애 때문에 어떤 곳에서던 쓰기 요청이 들어올 경우, n1, n2, n3는 다른 일관성을 가지게 된다. 

일관성을 선택할 경우, 데이터 불일치 문제를 해결하기 위해, 쓰기 연산이 작동하지 않는다. 이런 시스템이라면 장애가 해결될 때 까지 쓰기 요청에 대해 오류를 반환해야 한다.

## 시스템 컴포넌트

키-값 저장소 구현에 사용될 핵심 컴포넌트들 및 기술들을 살펴보자.

### 데이터 파티션

데이터를 작은 파티션들로 분할한 다음 여러 서버에 저장하는 방식이다. 데이터 파티션을 이용할 때는 다음 문제를 중요하게 따져봐야 한다.

- 데이터를 고르게 분산할 수 있는지
  
- 서버가 추가, 삭제될 때, 데이터의 이동을 최소화할 수 있는지

안정 해시는 이 문제를 해결하는데 적합한 기술이다. 안정 해시의 사용은 다음과 같은 이점을 가진다.

- 규모 확장 자동화 : 시스템 부하에 따라 서버를 유동적으로 조절할 수 있다.
  
- 다양성 : 각 서버의 용량에 맞게 가상 노드를 조정할 수 있다. 즉, 고성능 서버에는 더 많은 가상 노드를 배치할 수 있다.

### 데이터 다중화

높은 가용성과 안정성을 위해서는 데이터를 N개의 서버에 비동기적으로 다중화할 필요가 있다.(같은 데이터를 여러 서버에 복제한다는 의미) N값은 튜닝이 가능하며, 안정 해시에서 시계 방향으로 돌 때 만나는 N개의 서버에 데이터 사본을 보관하는 방식이다.

그러나 가상 노드를 사용한다면 N개의 노드가 대응될 실제 물리 서버의 개수가 N보다 작아질 수 있다. 이 문제를 피하려면 중복 선택을 막으면 된다.

같은 데이터 센터에 속한 가상 노드는 자연재해 등의 문제를 동시에 겪을 가능성이 있다. 안정성을 위해 데이터의 사본은 다른 센터에 보관하고, 센터들은 고속 네트워크로 연결한다.(하나의 데이터 센터 내에서 데이터를 복제할 뿐만 아니라, 여러 데이터 센터에서 복제한다)

### 데이터 일관성

여러 노드에 다중화된 데이터는 적절히 동기화가 되어야 한다. 정족수 합의 프로토콜을 사용하면 읽기, 쓰기 연산 모두에 일관성을 보장할 수 있다.

N을 사본의 개수, W를 쓰기 연산에 대한 정족수, R을 읽기 연산에 대한 정족수라고 할 때, 다음이 성립된다.(W와 R의 정확한 정의는 각자의 연산이 성공한 것으로 간주되며, 적어도 W/R개의 서버로부터 응답을 받아야 함을 의미한다)

중재자는 클라이언트와 노드 사이에서 프록시 역할을 하며, W/R개 이상의 성공 응답을 받을 경우, 다음 과정을 진행하는 역할을 한다.

W, R, N의 값을 정하는 것은 응답 시간과 데이터 일관성 사이의 타협접을 정하는 과정이다. 

- W + R > N인 경우를 강한 일관성이라고 하는데, 적어도 하나의 노드에서는 읽기와 쓰기 요청이 성공함을 의미한다.(쓰기와 읽기 요청에서 데이터 복제가 성공했음을 의미한다.)
  
- R=1, W=N인 경우, 빠른 읽기 연산에 최적화된 시스템이다.
- W=1, R=N인 경우, 빠른 쓰기 연산에 최적화된 시스템이다.
- W + R ≤ N인 경우, 강한 일관성이 보장되지 않는다.

### 일관성 모델

일관성 모델은 데이터 일관성의 수준을 결정하며, 다양한 종류가 존재한다.

- 강한 일관성 : 모든 읽기 연산은 가장 최근에 갱신된 결과를 반환한다. (낡은 데이터를 못본다)
  
- 약한 일관성: 읽기 연산은 가장 최근에 갱신된 결과를 반환하지 못할 수 있다.
- 최종 일관성 : 약한 일관성의 한 종류로, 갱신 결과가 결국에는 모든 사본에 반영되는 모델이다.

강한 일관성 모델은 일반적으로 모든 사본에 현재 쓰기 연산 결과가 반영될 떄 까지 해당 데이터에 대한 읽기, 쓰기 요청을 금지시킨다. 이는 고가용성에 적합하지 않다.

최종 일관성 모델을 따를 경우, 쓰기 연산이 병렬적으로 발생하면 시스템에 저장된 값의 일관성이 깨질 수 있는데, 이 문제는 클라이언트가 버전 정보 활용 등 여러 기법을 통해 해결할 수 있다.

### 비 일관성 해소 기법 : 데이터 버저닝(data versioning)

데이터를 다중화하면 가용성은 높아지지만 일관성이 깨질 수 있다. 이를 버저닝과 벡터 시계로 해결할 수 있다. 버저닝은 데이터를 변경할 때 마다 새로운 버전을 만드는 것을 의미한다. 따라서, 각 버전의 데이터는 변경 불가능하다.

다중화된 서버에 각각 다른 값을 동시에 입력할 때, 충돌이 발생하게 된다.

벡터 시계는 [서버, 버전]의 순서쌍을 데이터에 추가한 것이다. 어떤 버전이 선행, 후행인지, 아니면 다른 버전과 충돌이 있는지 판별한다. 벡터 시계는 `D([S1, v1]. [S2, v2], …)`와 같이 표현된다. 여기서 D는 데이터를 의미하며 Si는 해당 데이터가 변경된 서버 위치, vi는 버전 카운터를 의미한다. 만약 데이터 D를 서버 Si에 기록하면 다음 중 하나를 수행해야 한다.(특정 서버가 데이터를 바꿨으면 [특정 서버, 변경 횟수]가 데이터에 추가된다는 뜻)

- [Si, vi]가 있으면 vi를 증가시킨다.
  
- 그렇지 않으면 새 항목 [S1, 1]를 만든다.

![image](https://github.com/user-attachments/assets/df66ea23-7a66-4a04-8183-45a283c917ce)

1. D라는 데이터에 대해 Sx 서버가 D1으로 변경할 경우, 백터 시계가 D1([Sx, 1])이 된다.
   
3. D1 데이터를 Sx 서버가 D2로 변경할 경우, 벡터 시계를 확인하고 카운터를 올려 D2([Sx, 2])가 된다. 
4. D2에 대해 Sy가 쓰기 작업을 할 경우, [Sy, 1]가 추가되어 D3([Sx, 2], [Sy, 1])가 된다.
5. D2에 대해 Sz가 쓰기 작업을 할 경우, [Sz, 1]가 추가되어 D4([Sx, 2], [Sz, 1])가 된다.
6. Sx 서버는 충돌을 감지하고 벡터 시계를 다음과 같이 변경한다 D5([Sx, 2], [Sy, 1], [Sz, 1])

버전 사이에서 충돌을 감지하려면, 벡터 시계의 각 요소를 비교하며 크기 순서가 일관적인지 확인하면 된다. 그러나 벡터 시계는 다음과 같은 단점을 가지낟.

- 충돌 감지 로직이 클라이언트 책임이므로, 클라이언트 구현이 복잡해진다.
  
- 벡터 시계의 크기가 굉장히 빠르게 늘어난다는 것이다. 이를 해결하기 위해 길이가 임계치를 넘으면 오래된 순서쌍은 삭제한다. 이는 삭제로 인한 선후 관계를 결정할 수 없다는 문제가 있지만, 보통 잘 일어나지 않는다.

### 장애 처리

대규모 시스템에서 장애는 흔한 일이다. 따라서, 이를 어떻게 처리하느냐가 굉장히 중요하다. 장애 감지 기법과 장애 해소 전략들을 확인하자.

### 장애 감지

분산 시스템에서 가십 프로토콜과 같은 장애 감지를 효율적으로 확인할 수 있다. 

가십 프로토콜 작동 방식은 다음과 같다. 각 노드는 (멤버 ID, 박동 카운터)로 이루어진 멤버십 목록을 유지한다. 각 노드는 주기적으로 박동 카운터를 증가시키며, 무작위로 노드들에게 자신의 박동 카운터를 전송한다. 반복되는 과정에서 일정 시간 기록된 박동 카운터가 변경되지 않는 경우, 해당 노드를 장애 상태로 간주한다.

### 일시적 장애 처리

장애를 감지한 시스템은 가용성 보장을 위해 엄격한 정족수 혹은 느슨한 정족수 접근법을 이용하여 조치를 취한다. 일시적 장애 처리기 때문에 영구적으로 부숴진 서버에 대해서는 다른 대책이 필요하다.

업격한 정족수 접근법은 읽기와 쓰기 요청을 중단시켜 문제를 해결한다. 그러나 이는 가용성에 좋지 못하다.

느슨한 정족수 접근법은 정족수 요구사항을 강제하는 대신, 읽기 및 쓰기 연산에 수행할 W/R개의 건강한 서버를 해시 링에서 고른다.

단서 후 임시 위탁 기법은 장애 서버로 가는 요청을 다른 서버가 위임해 처리하는 방식이다. 쓰기 요청의 경우 단서를 남겨두며, 장애 서버가 복구되면 일괄 반영하여 데이터 일관성을 보존한다. 해당 기법은 해시 링에서 다음 서버가 맡아 처리한다.

![image](https://github.com/user-attachments/assets/1a31ed45-e356-4c38-ab74-32b3a71d95ad)

### 영구 장애 처리

반 엔트로피 프로토콜을 이용하여 영구적인 장애 상태를 처리할 수 있다. 반 엔트로피 프로토콜은 사본들을 비교하여 최신 버전으로 갱신하는 과정을 포함한다. 사본간의 일관성이 망가진 상태를 탐지하고 전송 데이터 양을 줄이기 위해선 머클 트리를 사용한다.

머클 트리란, 각 노드에 그 자식 노드들에 보관된 값의 해시, 또는 지식 노도들의 레이블로부터 계산된 해시값을 레이블로 붙여두는 트리다. 머클 트리를 사용하면 대규모 자료구조의 내용을 효과적이면서도 보안상 안전한 방법으로 검증할 수 있다.

과정은 다음과 같다.

1. 서버에 존재하는 키 공간을 일정 크기의 버킷으로 나눈다.
   
   ![image](https://github.com/user-attachments/assets/723c48d9-f745-4dbb-9016-0276c7e22fe9)

3. 버킷에 포함된 각각의 키에 균등 분포 해시 함수를 적용하여 해시 값을 계산한다.(각각의 키를 해시값으로 변경한다)
   
   ![image](https://github.com/user-attachments/assets/5b693f46-1b48-485a-b0e2-7ee61e271f13)

5. 버킷별로 해시값을 계산한 후, 해당 해시 값을 레이블로 갖는 노드를 만든다.(변경된 해시를 아우르는 하나의 해시값을 만든다)
   
   ![image](https://github.com/user-attachments/assets/1d534c4e-8799-40c0-8e1d-c434ce431cf0)

7. 자식 노드의 레이블로부터 새로운 해시 값을 계산하여, 이진 트리를 상향식으로 구성한다.(해당 해시값을 조합하여 새로운 부모 노드를 만든다는 의미)
   
   ![image](https://github.com/user-attachments/assets/8cce010f-b3f6-4123-b823-dfdae376679b)

### 데이터 센터 장애 처리

자연 재해에 대응할 수 있도록 데이터를 여러 데이터 센터에 다중화하는 것이 중요하다.

### 시스템 아키텍쳐 다이어그램

키-값 저장소에 대한 아키텍쳐 다이어그램을 그려보자. 기능은 다음과 같다.

- 클라이언트는 키-값 저장소가 제공하는 두 가지 단순한 API와 통신한다
  
- 중재자는 클라이언트에게 키-값 저장소에 대한 프록시 역할을 하는 노드다.
- 노드는 안정 해시의 해시 링 위에 분포한다.
- 노드를 자동으로 추가 및 삭제할 수 있도록, 시스템은 완전히 분산된다.
- 데이터는 여러 노드에 다중화된다.
- 모든 노드가 같은 책임을 지므로, SPOF는 존재하지 않는다.

### 쓰기 경로

다음은 쓰기 요청의 흐름이다.

![image](https://github.com/user-attachments/assets/3752f3ea-a4b9-49f6-b54f-bb42a28651b3)


1. 쓰기 요청이 커밋 로그 파일에 기록된다.
   
3. 데이터가 메모리 캐시에 기록된다.
4. 메모리 캐시가 가득 차거나, 정의된 임계치에 도달하면 데이터는 디스크에 있는 SSTable에 기록된다. SSTable은 키-값 순서쌍을 정렬된 리스트 형태로 관리하는 테이블이다.

### 읽기 경로

다음은 읽기 요청의 흐름이다. 읽기 요청을 받은 노드는 데이터가 메모리 캐시에 있는지부터 살핀다.

![image](https://github.com/user-attachments/assets/f2114e4f-83b9-4f71-bb5c-122e0c534477)

메모리에 데이터가 없다면 디스크에서 가져와야 한다. 블룸 필터는 디스크(SSTable)에서 키를 찾는데 효과적인 방법이다. 구체적은 흐름은 다음과 같다.

![image](https://github.com/user-attachments/assets/229f3932-d3c7-4731-8dca-d7ed9b9c8513)

1. 데이터가 메모리에 있는지 검사한다
   
3. 데이터가 메모리에 없다면 블롬 필터를 검사한다
4. 블룸 필터를 통해 어떤 디스크에 키가 보관되어 있는지 확인한다.
5. 테이블에서 데이터를 가져온다.
6. 해당 데이터를 클라이언트에 반환한다.

### 블룸 필터

블룸 필터란 특정 원소가 집합에 속하는지 검사하는 데 사용되는 확률형 자료구조다. 

데이터를 입력할 때, 해당 데이터를 k개의 해시 함수에 각각 대입한 뒤 나온 결과를 비트 배열에 저장한다. 데이터를 확인할 때, 해당 데이터를 해시 함수에 대입한 뒤 나온 결과를 비트 배열에서 확인한다.

블룸 필터는 데이터의 존재 여부를 확인하는 과정이 빠르다는 장점이 있다. 그러나 거짓 양성 가능성이 존재한다는 단점도 있다. 디스크에 데이터가 존재하는지 확인하는 전처리 과정으로 사용할 수 있다.
