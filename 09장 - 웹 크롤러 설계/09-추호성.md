# 9. 웹 크롤러 설계

웹 크롤러는 검색 엔진에서 널리 쓰이는 기술로, 웹에 새로 올라오거나 갱신된 콘텐츠를 찾아내는 것이 주 목적이다. 웹 크롤러는 몇 개의 웹 페이지에서 시작하여 그 링크를 따라나가면서 새로운 컨텐츠를 수집한다. 웹 크롤러는 검색 엔진 인덱싱, 웹 아카이빙, 웹 마이닝, 웹 모니터링 등으로 이용된다.

그중, 검색 엔진 크롤러는 웹사이트를 자동으로 탐색하고 데이터를 수집하는 프로그램이다. 구글, 네이버 같은 검색 엔진은 웹 크롤러를 사용하여 웹페이지 정보를 수집하고, 이를 검색 인덱스에 저장한 후 검색 결과로 제공한다.

<br>

## 1. 문제 이해 및 설계 범위 확정

웹 크롤러의 기본 알고리즘은 다음과 같다.

1. URL 집합의 모든 웹 페이지를 다운로드한다.

2. 다운받은 웹 페이지에서 URL을 추출한다.

3. 추출된 URL들을 다운로드할 URL 목록에 추가하고 위 과정을 반복한다.

그러나 거대한 확장성을 갖는 웹 크롤러를 설계하는 것은 어려운 일이다. 먼저 면접관에게 질문을 던져 요구사항을 구체화하자. 요구 사항은 다음과 같다.

- 검색 엔진 인덱싱에 사용

- 10억 개 / Month 웹사이트 수집

- 새로운 웹 페이지나 수정된 웹 페이지 고려

- 수집한 웹 페이지는 5년간 저장

- 중복된 컨텐츠는 무시

이 뿐만 아니라, 크롤링의 기본적인 속성을 지켜야 한다.

- 규모 확장성 : 웹 페이지가 많기 때문에, 병렬을 이용하여 효과적으로 문제를 해결하자

- 안정성 : 웹은 함정이 가득하다(악성 코드, 장애, 반응 없는 서버 등) 크롤러는 이러한 비정상적 입력에 잘 대처할 수 있어야 한다.

- 예절 : 크롤러는 기존 웹사이트에 요청을 보내는 방식으로 작동한다. 과도한 요청을 보내선 안된다

- 확장성 : 이미지 크롤링 등 새로운 형태의 컨텐츠를 지원하기 쉬워야 한다.


<br>

### 개략적 규모 추정

- 매달 10억개 페이지를 다운로드 한다.

- 이는 400 QPS (초당 400 페이지 다운) → 최대 800QPS

- 페이지 크기가 500K라고 하면 월 500TB가 요구되며, 5년간 30PB가 요구된다.

<br>

## 2. 개략적 설계안 제시 및 동의 구하기

하단 다이어그램에 등장하는 컴포넌트의 기능을 이해해보자.

![image (4)](https://github.com/user-attachments/assets/20ec5114-780f-46ef-9b50-0c4dcd2b3e6c)

### 시작 URL 집합

웹 크롤러의 시작점이다. 시작 URL을 잘 선정해야 전체 웹 크롤링이 원활하게 이뤄진다. 일반적으로 전체 URL 공간을 작은 부분집합으로 나누는 전략 혹은 주제별로 다른 시작 URL을 사용하는 전략을 사용한다.

전체 URL 공간을 작은 부분집합으로 나누는 전략이란, URL 모음을 의미있는 기준으로 분리한다는 뜻인가?

주제별로 다른 시작 URL을 사용하는 전략은 스포츠, 쇼핑 등 주제별로 다른 시작 URL을 사용하는 것을 의미한다. 이는 겹치지 않는 주제를 선정하여 크롤링 효율을 높임을 의미할수도..?

<br>

### 미수집 URL 저장소

대부분의 웹 크롤러는 크롤링 상태를 다운로드할 URL, 다운로드된 URL 두 가지로 나눠 관리한다. 이 중 다운로드 예정인 URL을 저장 관리하는 컴포넌트를 미수집 URL 저장소라고 부른다. 큐 자료구조를 이용한다.

<br>

### HTML 다운로더

HTML 다운로더는 인터넷 웹페이지를 다운로드하는 컴포넌트다. 

<br>

### 도메인 이름 변환기

웹 페이지를 다운받으려면 URL을 IP 주소로 변환하는 절차가 필요하다. 도메인 이름 변환기는 이를 도와준다.

<br>

### 컨텐츠 파서

웹 페이지를 다운로드하면 파싱과 검증 절차를 거쳐야 한다. 이상한 웹 페이지는 문제를 일으킬 수 있으며, 저장 공간만 낭비하게 되기 때문이다. 크롤링 서버 안에 파서를 구현하면 크롤링 과정이 느려질 수 있기 때문에 독립적인 컴포넌트로 구성했다.

<br>

### 중복 컨텐츠 여부

중복된 페이지를 구별하기 위해 자료 구조를 도입하여 데이터 처리에 소요되는 시간을 줄인다. 두 웹페이지가 같은지 비교하는 효과적인 방법으로는 해시가 있다.

<br>

### 컨텐츠 저장소

HTML 문서를 보관하는 시스템이다. 저장소를 구현하는 데 쓰일 기술을 고를 때는 저장할 데이터의 유형, 크기, 저장소 접근 빈도 등을 종합적으로 고려해야 한다. 본 설계안의 경우 디스크와 메모리를 동시에 사용하여 저장한다. 기본적으로 데이터의 양이 많으므로 대부분의 컨텐츠는 디스크에 저장하며, 인기 있는 컨텐츠는 메모리에 둔다.

<br>

### URL 추출기

URL 추출기는 HTML 페이지를 파싱하여 링크를 골라내는 역할을 한다.

<br>

### URL 필터

특정 컨텐츠, 파일 확장자를 가지는 URL, 접속 오류가 발생하는 URL, 접근 제외 목록에 포함된 URL 등을 크롤링 대상에서 배제하는 역할을 한다.

<br>

### URL 방문 여부 (중복 URL 판별기)

이미 방문한 URL을 추적하면 URL 중복을 피할 수 있으므로 서버 부하를 줄이고 시스템이 무한 루프에 빠지는 것을 막아준다. 이를 판별하기 위한 자료구조로 블룸 필터나 해시 테이블이 존재한다.

<br>

### URL 저장소

이미 방문한 URL을 보관하는 저장소다.

<br>

### 웹 크롤러 작업 흐름

다음은 웹 크롤러의 작업 흐름이다.

![image (5)](https://github.com/user-attachments/assets/61e44981-df53-4d80-a580-2e386ebef932)

1. 시작 URL들을 미수집 URL 저장소에 저장한다.

2. HTML 다운로더는 미수집 URL 저장소에서 URL 목록을 가져온다. 
3. HTML 다운로더는 도메인 이름 변환기를 사용하여 URL 을 IP 주소 변환 및 접속하여 웹 페이지를 가져온다.
4. 컨텐츠 파서는 다운된 HTML 페이지를 파싱하여 올바른 형식을 갖춘 페이지인지 검증한다.
5. 컨텐츠 파싱과 검증이 끝나면 중복 여부를 확인한다.
6. 해당 페이지가 이미 저장소에 존재하는지 확인하여 중복 여부를 확인한다.
    - 이미 저장소에 있다면 버린다.

    - 그렇지 않다면 저장소에 저장한 뒤 URL 추출기로 전달한다.
7. 파싱된 페이지에서 URL을 추출한다.
8. 골라낸 URL을 필터에 전달한다.
9. URL 필터는 필터링을 통해 특정 컨텐츠, 파일 확장자 등을 거르며 남은 URL을 중복 URL 판별기로 전달한다.
10. 중복 URL이 맞는 경우, 해당 URL을 버린다.
11. 그렇지 않은 경우 URL 저장소, 미수집 URL 저장소에 저장한다.

<br>

## 3. 상세 설계

중요한 컴포넌트와 구현 기술을 탐구해보자.

<br>

### DFS vs BFS

DFS는 깊이 우선 탐색법으로 하나의 웹 페이지에 존재하는 URL을 끝까지 탐색하는 방식을 이용하는데, 웹 페이지의 깊이를 가늠할 수 없으므로 DFS보단 BFS를 추천한다.

BFS는 넓이 우선 탐색법으로 하나의 웹 페이지를 시작으로 하나의 DEPTH에 존재하는 다양한 웹 페이지를 탐색하는 방식이다. 그러나 이는 하나의 웹 서비스에 큰 부하를 줄 수 있다. 이를 예의 없는 크롤러라고 한다.

표준적 BFS 알고리즘은 URL에 우선순위를 두지 않지만, 페이지 순위, 사용자 트래픽 양, 업데이트 빈도 등 여러 기준에 비추어 처리 우선순위를 구별하는 것이 옳다.

<br>

### 미수집 URL 저장소

미수집 저장소를 이용한다면 예의 장착, URL 사이의 우선순위와 신선도를 구별할 수 있으며, 이를 통해 기존 문제를 해결할 수 있다. 미수집 URL 저장소의 구현 방법은 다음과 같다.

<br>

### 미수집 URL 저장소 구현 방법 - 예의

동일한 웹 사이트에 대해 한 번에 한 페이지만 요청하는 것이 예의다. 같은 웹 사이트의 페이지를 다운 받는 작업은 시간차를 두고 실행시키면 된다. 이를 만족시키려면 웹사이트의 호스트명과 다운로드를 수행하는 작업 스레드 사이의 관계를 유지하면 된다. 다음 설계를 참고하자.

![image (6)](https://github.com/user-attachments/assets/544d8c80-b15a-4977-aad3-f6d17d8d66fe)

- 큐 라우터 : 같은 호스트에 속한 URL은 언제나 같은 큐로 가도록 보장한다.

- 매핑 테이블 : 호스트 이름과 큐 사이의 관계를 보관하는 테이블
- 삐뽀 큐 : 같은 호스트에 속한 URL은 언제나 같은 큐에 보관된다.
- 큐 선택기 : 큐를 순회하면서 URL을 꺼내며, 지정된 작업 스레드에게 전달한다.
- 작업 스레드 : 전달된 URL을 다운로드하는 작업을 수행한다. 전달된 URL은 순차적으로 처리되며, 작업들 사이에는 일정 지연시간을 둘 수 있다.

이를 종합했을 때, 각각의 큐는 호스트명을 기반으로 분류되며, 특정 작업 스레드와 매핑되어 있다. 작업 스레드는 URL을 일정 지연시간을 두고 처리하기 때문에, 특정 호스트명을 가진 URL을 예의 있게 처리할 수 있다.

<br>

### 미수집 URL 저장소 구현 방법 - 우선순위

페이지를 랭크, 트래픽 양, 갱신 빈도 등 다양한 기준으로 우선순위를 결정하는 역할을 한다. 우선순위를 결정하여 이를 큐에 배치한다.

![image (7)](https://github.com/user-attachments/assets/5fc4bddb-e412-418c-bb5d-00bf5239a579)

- 순위 결정 장치 : URL을 입력받아 우선순위를 계산하여 알맞는 큐를 배정한다.

- 큐 : 우선순위별로 할당된다(높음, 중간, 낮음 등)
- 큐 선택기 : 임의 큐에서 처리할 URL을 꺼내는 역할을 한다. 순위가 높은 큐에서 더 자주 꺼낸다.

두 개의 모듈을 전면 큐, 후면 큐로 연결할 경우, 다음과 같이 배치된다. 다음 구현 방식은 입력된 URL에 대해 우선순위를 결정하고, 이를 도메인 별로 나누어 예의 있게 처리하는 과정을 가진다.

![image (8)](https://github.com/user-attachments/assets/c3ae6255-35d9-4c73-85ff-bdfe27e04f51)

<br>

### 신선도

웹 페이지는 수시로 변경된다. 데이터의 신선함을 유지하기 위해서는 이미 다운로드된 페이지라고 해도 재수집할 필요가 있다. 모든 페이지를 재수집할 수 없으므로 다음과 같은 방법을 사용할 수 있다.

- 웹 페이지 변경 이력을 활용한다.

- 재수집 빈도에 우선순위를 둔다.

<br>

### 미수집 URL 저장소를 위한 지속성 저장장치

검색 엔진을 위한 크롤러의 경우, 처리해야 할 URL이 수억 개가 되기 때문에, 모든걸 메모리에 담을 수 없다. 그러나 전부 디스크에 담으면 병목이 생길 수 있기 때문에, 메모링 버퍼에 큐를 두어 주기적으로 디스크에 기록하며 IO 비용을 줄인다.

<br>

### HTML 다운로더

HTML 다운로더는 HTTP 프로토콜을 통해 웹 페이지를 내려받는다. 먼저 로봇 제외 프로토콜에 대해 알아보자.

<br>

### Robots.txt

Robots.txt는 웹 사이트와 크롤러가 소통하는 표준적 방법이다. 크롤러는 웹 사이트를 긁어 가기 전에 해당 파일에 기재된 규칙을 확인해야 한다. 다음은 아마존의 Robot.txt인데, creatorhub 디렉터리에 존재하는 내용은 긁어올 수 없음을 의미한다.

![image (9)](https://github.com/user-attachments/assets/c10b4f14-f181-4df8-a283-fb6c1803f164)

<br>

### 성능 최적화

다음은 HTML 다운로더에 사용할 수 있는 성능 최적화 기법이다.

<br>

### 성능 최적화 - 분산 크롤링

성능을 높이기 위해 여러 서버에서 분산 크롤링을 진행한다. 미수집 URL 저장소에 있는 URL을 작은 단위로 분할하여 각각의 다운로드 서버에 배정하는 방식이다. 각 스레드가 분할된 URL 모음을 담당한다.

![image (10)](https://github.com/user-attachments/assets/4ede96fc-1f59-4736-b00b-3c935327882c)

<br>

### 성능 최적화 - 도메인 이름 변환 결과 캐시

도메인 이름 변환기는 DNS에 요청을 보내고 결과를 받는 작업의 동기적 특성 때문에 병목 요인 중 하나다. 이를 해결하기 위해선 도메인 이름 저장 캐시를 두어, IP와 도메인 주소 관계를 보관한 뒤, 크론 잡 등을 돌려 주기적으로 갱신시킨다.

(크론 잡은 리눅스 및 유닉스 시스템에서 특정 시간 간격마다 자동으로 실행되는 작업을 의미한다)

<br>

### 성능 최적화 - 지역성

크롤링 작업을 수행하는 서버를 지역별로 분산하는 방법이다. 가까운 웹 서버에 대해 크롤링한다면 더욱 높은 속도를 가질 수 있다. 이는 서버 뿐만 아니라 캐시, 큐, 저장소 등 대부분의 컴포넌트에 적용 가능하다.

<br>

### 성능 최적화 - 짧은 타임아웃

특정 웹서버는 응답이 느리거나 응답하지 않는다. 이런 경우, 대기 시간이 길어지고 성능 약화를 야기하는데, 이를 짧은 타임아웃을 두어 개선할 수 있다.

<br>

### 안정성

최적화 뿐만 아니라 안정성도 다운로더 설계 시에 중요하다. 안정성을 보장하기 위한 다양한 방법이 있다.

- 안정 해시 : 다운로드 서버가 분산 환경일 경우, 안정 해시를 통해 부하를 고르게 분배할 수 있다. 또한 다운로드 서버의 확장과 축소에서도 유리하다.

- 크롤링 상태 및 수집 데이터 저장 : 장애가 발생할 경우, 이를 복구할 수 있도록 크롤링 상태와 수집된 데이터를 자속적 저장장치에 기록하는게 바람직하다.
- 예외 처리 : 예외가 발생하도 전체 시스템 중단 없이 크롤링을 쉽게 재시작 할 수 있어야 한다.
- 데이터 검증 : 올바른 HTML 파일을 저장하며, 시스템 오류를 방지하기 위해 데이터 검증이 필요하다.

<br>

### 확장성

크롤링 서비스에서도 확장성이 필요하다. 요구사항이 변경될 경우, 서비스를 쉽게 변경할 수 있도록 유연하게 설계해야 한다.

다음은 URL 뿐만 아니라 PNG 파일을 크롤링 할 수 있으며, 저작권이 침해될 수 없도록 웹을 모니터링하는 기능을 추가한 아키텍쳐다.

![image (11)](https://github.com/user-attachments/assets/345f808f-d14a-4116-9546-35b80cfb9a05)

<br>

### 문제 있는 컨텐츠 감지 및 회피

중복 컨텐츠는 해시나 체크섬을 사용하여 탐지할 수 있다.

두 페이지를 번갈아 참조하며 크롤러를 무한루프에 빠질 수 있게하는 링크가 존재할 수 있다. 이를 거미 덫이라고 하는데, URL 최대 길이 제한으로 해결할 수 있다. 그러나 이는 시작에 불과하며 거미 덫과의 싸움은 창과 방패의 싸움과 같다. 한 가지 방법은 사람이 수작업으로 덫을 확인하고 제거하는 방법이다.

광고, 스팸 URL 등 가치 없는 컨텐츠를 제외시켜야 한다.

<br>

## 4. 마무리

좋은 크롤링 서비스는 규모 확장성, 예의, 확장성, 안정성을 갖춰야 한다.

시간이 남는다면 다음과 같은 문제를 논의해보자.

- 서버 측 렌더링 : 서버 측 렌더링 적용을 통해 동적으로 만들어진 링크를 감지한다.

- 원치 않는 페이지 필터링 : 스팸 방지 컴포넌트를 두어 잘못된 페이지를 걸러내야 한다. 어떻게?
- 데이터베이스 다중화 및 샤딩 : URL 저장소, 컨텐츠 저장소와 같은 DB를 다중화하는 게 안정성에 좋다.
- 수평적 규모 확장성 : 다중화에서 중요한 것은 서버의 무상태성이다. 이를 지키도록 만들자.
- 가용성, 일관성, 안정성 : 다음 개념이 지켜져야 한다.
- 데이터 분석 솔루션 : 서비스를 세밀하게 조정하기 위해 데이터를 수집하고 분석하는 시스템을 만들자.
